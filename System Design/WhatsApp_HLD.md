 **WhatsApp High-Level Architecture (HLD)** for **100M+ concurrent users**.

This is how you answer in senior interviews.

---

# ğŸŸ¢ 1ï¸âƒ£ Goals of WhatsApp HLD

System must support:

* 1â€“1 Messaging
* Group Messaging
* Status
* Audio/Video Calls
* End-to-End Encryption
* 100M+ concurrent users
* Low latency (<100ms message delivery)
* High availability (99.99%)

---

# ğŸ— 2ï¸âƒ£ High-Level Architecture Overview

![Image](https://www.infosys.com/content/dam/infosys-web/en/techcompass/images/distributed-messaging-systems06.jpg)

![Image](https://miro.medium.com/1%2AgW4JrHTr86HnTrouQYLgJQ.png)

![Image](https://i.sstatic.net/7nPwb.png)

![Image](https://media2.dev.to/dynamic/image/width%3D800%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A%2F%2Fraw.githubusercontent.com%2Fsw360cab%2Fwebsockets-scaling%2Fmaster%2Fdiagrams%2Fclustered_k8s_nodeport.png)

---

# ğŸ§± 3ï¸âƒ£ Core Components

```
Clients (Mobile/Web)
        â†“
Global Load Balancer
        â†“
API Gateway
        â†“
Auth Service
        â†“
Real-Time Messaging Service (WebSocket Cluster)
        â†“
Message Queue (Kafka)
        â†“
Message Service
        â†“
Databases + Cache
        â†“
Notification Service
```

---

# 4ï¸âƒ£ Client Layer

* Android / iOS / Web
* Persistent WebSocket connection
* Local encryption/decryption
* Stores session keys locally

---

# 5ï¸âƒ£ Global Load Balancer

Distributes traffic across regions.

Used:

* Geo-DNS
* Anycast routing

Ensures:

* Lowest latency region routing
* Failover handling

---

# 6ï¸âƒ£ API Gateway

Responsibilities:

* Rate limiting
* Authentication validation
* Routing to microservices
* Metrics collection

Acts as:
**Single entry point**

---

# 7ï¸âƒ£ Authentication Service

* Validates JWT
* Manages sessions
* Handles device binding
* Key bundle distribution (for E2EE)

---

# 8ï¸âƒ£ Real-Time Messaging Layer

Core of WhatsApp.

Uses:

* WebSocket cluster
* Sticky sessions

Each server maintains:

```
userId â†’ socket connection map
```

When message arrives:

1. Check if receiver online
2. Push via WebSocket
3. If offline â†’ enqueue

---

# 9ï¸âƒ£ Message Queue (Kafka)

Used for:

* Decoupling real-time layer
* Reliability
* Message durability
* Replay in case of failure

Flow:

```
WebSocket â†’ Kafka â†’ Message Service â†’ DB
```

Benefits:

* Handles spikes
* Prevents overload
* Enables async processing

---

# ğŸ”Ÿ Message Service

Responsible for:

* Persisting encrypted messages
* Updating delivery status
* Group fan-out logic
* Triggering notifications

---

# 1ï¸âƒ£1ï¸âƒ£ Database Design (At Scale)

We use **polyglot persistence**.

## Primary DB (Messages)

* Cassandra / DynamoDB
* Partitioned by chatId
* Optimized for write-heavy workload

Why NoSQL?

* Massive scale
* Horizontal scalability
* High availability

---

## User Metadata DB

* MySQL / PostgreSQL
* Strong consistency

---

## Cache Layer

* Redis

Used for:

* Online users
* Session tokens
* Rate limiting
* Presence info

---

# 1ï¸âƒ£2ï¸âƒ£ Media Storage

Images/videos stored in:

* Object storage (S3 equivalent)
* CDN for global delivery

Flow:

```
Client encrypts media
Upload to storage
Store media URL in message
```

---

# 1ï¸âƒ£3ï¸âƒ£ Status Service

Separate microservice.

* TTL index (24hr expiry)
* Cached feed generation
* Privacy filtering

---

# 1ï¸âƒ£4ï¸âƒ£ Call System HLD

![Image](https://webrtc.github.io/webrtc-org/assets/images/webrtc-public-diagram-for-website.png)

![Image](https://runby.com/system/inserted_images/images/000/000/583/blog_export/TURN.png?1488875881=)

![Image](https://blog.ivrpowers.com/postimages/technologies/ivrpowers-turn-stun-screen.005.jpeg)

![Image](https://storage.googleapis.com/100ms-cms-prod/cms/turn_server_usage_6868821b66/turn_server_usage_6868821b66.jpg)

Uses:

* Signaling Server (WebSocket)
* WebRTC for media
* STUN/TURN servers

Flow:

```
Caller â†’ Signaling Server â†’ Receiver
      â†“
Offer/Answer exchange
      â†“
Direct peer-to-peer media
```

Media does NOT pass through WhatsApp servers.

---

# 1ï¸âƒ£5ï¸âƒ£ Group Messaging at Scale

Problem:

* 1 message â†’ 1000 users

Solution:

Fan-out on write:

```
Sender
  â†“
Kafka
  â†“
Generate copies per user
  â†“
Store in user inbox partitions
```

Scales better than fan-out on read.

---

# 1ï¸âƒ£6ï¸âƒ£ End-to-End Encryption Layer

Encryption occurs:

* On client
* Before network transmission

Server stores:

* Ciphertext only
* No plaintext
* No private keys

Key distribution service:

* Public keys only

---

# 1ï¸âƒ£7ï¸âƒ£ Presence System

Tracks:

* Online / Offline
* Typing indicator
* Last seen

Stored in:

* Redis (in-memory)
* TTL-based expiration

---

# 1ï¸âƒ£8ï¸âƒ£ Scalability Strategy

### Horizontal Scaling

* Stateless services
* Auto-scaling groups
* Kubernetes orchestration

### Data Partitioning

* Shard by userId
* Shard by chatId

---

# 1ï¸âƒ£9ï¸âƒ£ Reliability Mechanisms

* Multi-region deployment
* Data replication
* Circuit breakers
* Retry with exponential backoff
* Idempotent message IDs

---

# 2ï¸âƒ£0ï¸âƒ£ Observability

* Prometheus (metrics)
* ELK stack (logs)
* Distributed tracing
* Alerting system

---

# 2ï¸âƒ£1ï¸âƒ£ Security

* TLS everywhere
* E2EE
* Rate limiting
* DDoS protection
* Device binding

---

# 2ï¸âƒ£2ï¸âƒ£ Handling 100M Concurrent Users

Key optimizations:

* Persistent WebSocket (no polling)
* Efficient binary protocol
* Minimal payload size
* Sharded DB clusters
* Event-driven architecture
* CDN offloading for media

---

# ğŸ¯ Final HLD Architecture Summary

```
Clients
  â†“
Global LB
  â†“
API Gateway
  â†“
Auth
  â†“
WebSocket Cluster
  â†“
Kafka
  â†“
Message Service
  â†“
NoSQL DB
  â†“
Redis Cache
  â†“
Notification + Media CDN
```

---

# ğŸ§  How to Present in Interview

1. Start with requirements
2. Draw high-level flow
3. Explain scaling
4. Explain storage choice
5. Add reliability
6. Add security
7. Discuss tradeoffs

---
Now we enter **real distributed systems territory**.

Message storage is the hardest part of WhatsApp scale.

We design **Message Sharding Strategy for 100Mâ€“1B users**.

---

# ğŸ¯ 1ï¸âƒ£ Problem Statement

We must handle:

* Billions of messages per day
* Massive write-heavy workload
* Low-latency reads
* Group fan-out
* Multi-region replication
* No single point of failure

---

# ğŸ§± 2ï¸âƒ£ Why Sharding Is Mandatory

Single DB will fail due to:

* Write bottleneck
* Storage limits
* Lock contention
* Index explosion

So we **horizontally shard**.

---

# ğŸ§  3ï¸âƒ£ Sharding Strategy Options

## Option A â€” Shard by UserID

## Option B â€” Shard by ChatID

## Option C â€” Hybrid (Recommended)

We analyze each.

---

# ğŸ”¹ Option A: Shard by UserID

```
shard = hash(userId) % N
```

Each userâ€™s inbox stored in one shard.

### âœ… Pros

* Fast user message fetch
* Balanced distribution (if users uniform)

### âŒ Cons

* Group message duplication complex
* Heavy users create hotspots

---

# ğŸ”¹ Option B: Shard by ChatID

```
shard = hash(chatId) % N
```

Each chat lives fully in one shard.

### âœ… Pros

* Group messages easy
* All chat messages colocated

### âŒ Cons

* Large groups â†’ hotspot shard
* Uneven distribution

---

# ğŸ”¥ Option C (Production-Grade): Hybrid Model

We combine:

* **Chat-based partitioning**
* **User inbox indexing**

---

# ğŸŸ¢ 4ï¸âƒ£ Recommended Production Architecture

### Primary Partition Key â†’ `chatId`

### Secondary Index â†’ `userId`

We use something like:

* Cassandra
* DynamoDB

Schema:

```
Partition Key: chatId
Clustering Key: timestamp
```

This allows:

* Efficient range queries
* Ordered retrieval
* Append-only writes

---

# ğŸ§± 5ï¸âƒ£ Physical Shard Distribution

![Image](https://assets.digitalocean.com/articles/understanding_sharding/DB_image_3_cropped.png)

![Image](https://cassandra.apache.org/_/_images/diagrams/apache-cassandra-diagrams-01.jpg)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AChr0HbRtaRYZRtNB1mrh6Q.png)

![Image](https://i.sstatic.net/QgZA3.png)

We distribute shards across:

* Multiple nodes
* Multiple availability zones
* Multiple regions

Using **consistent hashing ring**.

---

# ğŸ” 6ï¸âƒ£ Consistent Hashing

Instead of:

```
hash(key) % N
```

We use:

* Hash ring
* Virtual nodes
* Minimal reshuffling when adding nodes

Benefits:

* Easy scaling
* Minimal data migration
* No downtime scaling

---

# ğŸ§  7ï¸âƒ£ Handling Group Messages at Scale

Problem:
Group of 10K users â†’ 1 message

Two strategies:

---

## Fan-Out on Write (WhatsApp uses this)

```
1 message â†’
   copy into each user's inbox
```

Stored as:

```
userId + messageId
```

### Why?

* Fast read
* No heavy aggregation during fetch

Tradeoff:

* Higher write amplification

At WhatsApp scale:
Reads > Writes â†’ So optimize reads.

---

# ğŸ§± 8ï¸âƒ£ Logical Data Model (Inbox Model)

Instead of storing single group copy:

We store:

```
UserInbox Table

Partition Key: userId
Clustering Key: timestamp
Columns:
  messageId
  chatId
  encryptedPayload
  status
```

Each user fetches from own shard.

---

# ğŸ”¥ 9ï¸âƒ£ Avoiding Hot Shards

We prevent hotspots via:

### 1ï¸âƒ£ Virtual Nodes

Each physical node hosts multiple token ranges.

### 2ï¸âƒ£ Dynamic Rebalancing

Monitor load â†’ move tokens.

### 3ï¸âƒ£ Rate Limiting Large Groups

Prevent spam storms.

---

# ğŸŒ 1ï¸âƒ£0ï¸âƒ£ Multi-Region Strategy

We use:

* Region-based sharding
* User affinity routing

Example:

```
India users â†’ India cluster
Europe users â†’ EU cluster
```

Messages cross-region via async replication.

---

# ğŸ” 1ï¸âƒ£1ï¸âƒ£ Replication Strategy

We use:

Replication Factor = 3

Across:

* AZ1
* AZ2
* AZ3

Ensures:

* High availability
* Read availability during node failure

---

# âš¡ 1ï¸âƒ£2ï¸âƒ£ Read Optimization

We use:

* Read replicas
* Redis cache for recent chats
* Pagination (limit 50 messages)

Query pattern:

```
SELECT * 
FROM UserInbox
WHERE userId = ?
ORDER BY timestamp DESC
LIMIT 50
```

Highly efficient in Cassandra.

---

# ğŸ“Š 1ï¸âƒ£3ï¸âƒ£ Write Optimization

Messages are:

* Append-only
* No updates (except status)
* No joins

This keeps:

* Write latency low
* Locking minimal

---

# ğŸ” 1ï¸âƒ£4ï¸âƒ£ Encryption Compatibility

Since payload encrypted:

DB does NOT need:

* Content indexing
* Text search

Only stores:

* Binary blobs

Improves performance.

---

# ğŸ§  1ï¸âƒ£5ï¸âƒ£ Scaling to 1 Billion Users

Key strategies:

* Auto-scaling DB nodes
* Partition key carefully chosen
* Compact storage format
* Background compaction
* Time-based archiving (cold storage)

Old messages â†’ moved to cheaper storage.

---

# âš– 1ï¸âƒ£6ï¸âƒ£ Tradeoffs Discussion (Interview Gold)

| Decision             | Tradeoff              |
| -------------------- | --------------------- |
| Fan-out on write     | High write cost       |
| NoSQL over SQL       | No joins              |
| Eventual consistency | Faster availability   |
| Shard by chatId      | Risk of group hotspot |
| Hybrid model         | More infra complexity |

---

# ğŸ§  1ï¸âƒ£7ï¸âƒ£ Failure Handling

If shard fails:

* Replica takes over
* Kafka replay restores missed writes
* Idempotent message IDs prevent duplicates

---

# ğŸ¯ Final Production Sharding Strategy

âœ” Partition by chatId
âœ” User-based inbox for fast read
âœ” Consistent hashing
âœ” Multi-AZ replication
âœ” Fan-out on write
âœ” Append-only model
âœ” Encrypted blob storage

---

# ğŸš€ How to Explain in Interview

Say:

> â€œAt WhatsApp scale, message storage is write-heavy and read-critical. I would use Cassandra with partition key as chatId, clustering by timestamp, and implement fan-out-on-write to user inbox tables. Shards would be distributed via consistent hashing with replication factor 3 across availability zones to ensure high availability and fault tolerance.â€

That sounds senior-level.

---

